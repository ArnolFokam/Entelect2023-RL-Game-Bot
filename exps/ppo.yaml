defaults:
  - spsa
  - _self_

# agent
agent_name: ppo
actor_learning_rate: 1e-4
critic_learning_rate: 1e-3
decay: 0.97 # general advantage estimate
ppo_clip_val: 0.2
target_kl_divergence: 0.03
max_actor_train_iterations: 5
critic_train_iterations: 10

# actor network architecture
actor_approximator: mlp
actor_num_hidden_layers: 3
actor_hidden_dim: 64

# value network architecture
critic_approximator: mlp
critic_num_hidden_layers: 1
critic_hidden_dim: 64